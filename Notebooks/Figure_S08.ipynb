{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "\n",
    "from datetime import datetime \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "cmap = cm.get_cmap('Blues')\n",
    "import random\n",
    "from scipy import signal, cluster, stats\n",
    "from multiprocessing import Pool\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy as sp\n",
    "from scipy import linalg\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import cairosvg\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_shifted_spline_frame(field, num):\n",
    "    #get data\n",
    "    pix_id = (NoiseSplineAlignment() & field).fetch('center_pixel_id')\n",
    "    shifted_filters =  (NoiseSplineAlignment() & field).fetch('shifted_filter')\n",
    "    z_position = (NoiseSplineRoiPosition() & field).fetch('z_position')\n",
    "    x_position = (NoiseSplineRoiPosition() & field).fetch('x_position')\n",
    "    depth = (NoiseSplineRoiPosition() & field).fetch('depth')\n",
    "\n",
    "    \n",
    "    #make frame\n",
    "    data = pd.DataFrame({'center_pixel_id':pix_id, 'shifted_filters':shifted_filters, \n",
    "                         'z_position':z_position, 'x_position':x_position, 'depth':depth,})\n",
    "    \n",
    "    \n",
    "    data['field'] = np.ones(data.shape[0])*num\n",
    "    data = data.set_index('center_pixel_id')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get original data to create features by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull these from hdf5\n",
    "s_df3 = pd.read_hdf('Data/Fig4.hdf5', 's_df3')\n",
    "s_df6 = pd.read_hdf('Data/Fig4.hdf5', 's_df6')\n",
    "s_df7 = pd.read_hdf('Data/Fig4.hdf5', 's_df7')\n",
    "s_df13 = pd.read_hdf('Data/Fig4.hdf5', 's_df13')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df_xz = pd.concat([s_df3, s_df6, s_df7, s_df13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#flattening\n",
    "splines = np.asarray(s_df_xz['shifted_filters'])\n",
    "spline_array = np.zeros((splines.shape[0], (splines[0].shape[0])*splines[0].shape[1]))\n",
    "spline_array_cropped = np.zeros((splines.shape[0], 212*7))\n",
    "\n",
    "for i, spline in enumerate(splines):\n",
    "    current_spline = spline\n",
    "    current_spline_flat = current_spline.flatten(order='F')\n",
    "    spline_array[i,:] = current_spline_flat\n",
    "    \n",
    "    current_spline_cropped = spline[50:-50,12:19] #fixing this to deal with the stimulator delay\n",
    "    current_spline_cropped_flat = current_spline_cropped.flatten()#order='F'\n",
    "    spline_array_cropped[i,:] = current_spline_cropped_flat\n",
    "\n",
    "# spline_array_cropped = spline_array[:, 2550:4200]##spline_array[:, 300:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#flattening the other way too\n",
    "splines = np.asarray(s_df_xz['shifted_filters'])\n",
    "\n",
    "spline_array_orderC = np.zeros((splines.shape[0], (splines[0].shape[0])*splines[0].shape[1]))\n",
    "\n",
    "for i, spline in enumerate(splines):\n",
    "    current_spline = spline\n",
    "    current_spline_flat = current_spline.flatten()#order='F')\n",
    "    spline_array_orderC[i,:] = current_spline_flat\n",
    "\n",
    "# spline_array_cropped = spline_array[:, 2550:4200]##spline_array[:, 300:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscored_spline_array = ((spline_array_cropped.T-np.mean(spline_array_cropped, axis=1))/np.std(spline_array_cropped, axis=1)).T\n",
    "\n",
    "threshold = 2.5\n",
    "quality_mask = np.where(np.amax(np.abs(zscored_spline_array), axis=1)>threshold)\n",
    "\n",
    "s_df_xz_masked = s_df_xz.iloc[quality_mask].copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = np.copy(spline_array_cropped[quality_mask[0],:])\n",
    "\n",
    "#sparsify the RFs\n",
    "transformer = SparsePCA(n_components=7, alpha=0.3, random_state=0) #, random_state=0 #(sets random seed)\n",
    "transformer.fit(x)\n",
    "x_transformed = transformer.transform(x)\n",
    "x_transformed.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_transformed_ipl = np.concatenate((x_transformed[:,:4], np.expand_dims(s_df_xz_masked['depth'], axis=1)), axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gm = GaussianMixture(n_components=13, random_state=0, covariance_type='diag',\n",
    "                    init_params='kmeans')\n",
    "fit = gm.fit(x_transformed_ipl)\n",
    "labels = gm.predict(x_transformed_ipl)\n",
    "s_df_xz_masked['new_cluster_assignment'] = labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#just a plot showing the normalized feature means for each cluster\n",
    "x_transformed_ipl_norm = x_transformed_ipl/np.amax(x_transformed_ipl, axis=0)\n",
    "\n",
    "for i in range(13):\n",
    "    plt.plot(np.mean(x_transformed_ipl_norm[labels==i], axis=0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classifiers = np.zeros((13,5))\n",
    "for i in range(13):\n",
    "    classifiers[i,:] = np.mean(x_transformed_ipl[labels==i], axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.imshow(classifiers/np.amax(classifiers, axis=0), aspect = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull from hdf5 file\n",
    "#read hdf5 file for this notebook\n",
    "f1 = h5py.File(\"Data/Fig4.hdf5\", \"r\")\n",
    "\n",
    "#get the data for plotting the sd image\n",
    "cluster_assignments = np.array(f1['cluster_assignments_original'])\n",
    "f1.close()\n",
    "\n",
    "s_df_xz_masked['new_cluster_assignment'] = cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the new data and sparsify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_for_duplicates(field):\n",
    "    center_px, roi_listed = (NoiseSplineROIs() & field).fetch('center_pixel_id', 'pixel_list')\n",
    "\n",
    "    new_roi_list = []\n",
    "    unique_center_pixels = []\n",
    "    duplicates = []\n",
    "    center_roi_duplicates = []\n",
    "    \n",
    "\n",
    "    for roi, roi_list in enumerate(roi_listed):\n",
    "        roi_list.sort()\n",
    "        if roi_list not in new_roi_list:\n",
    "            new_roi_list.append(roi_list)\n",
    "            unique_center_pixels.append(center_px[roi])\n",
    "        else:\n",
    "            duplicates.append(roi_list)\n",
    "            center_roi_duplicates.append(center_px[roi])\n",
    "    n_dups = len(center_roi_duplicates)\n",
    "\n",
    "    return unique_center_pixels, n_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_line(points):\n",
    "    x_coords, y_coords = zip(*points)\n",
    "    A = np.vstack([x_coords,np.ones(len(x_coords))]).T\n",
    "    m, c = lstsq(A, y_coords, rcond=None)[0]\n",
    "    \n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def norm(x): \n",
    "    # dividing by the norm = turning a RF into unit vector, \n",
    "    # not necessary, but for better visualization\n",
    "    return x / np.linalg.norm(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_spline_frame_pharma(field, pharma_field, num):\n",
    "    #get data\n",
    "    pix_id = (NoiseSplineRF2() & field).fetch('center_pixel_id')\n",
    "    sta =  (NoiseSplineRF2() & field).fetch('sta')\n",
    "    spl =  (NoiseSplineRF2() & field).fetch('spl')\n",
    "    \n",
    "    spl_pharma =  (NoiseSplineRF2() & pharma_field).fetch('spl')\n",
    "    \n",
    "    \n",
    "    #make frame\n",
    "    data = pd.DataFrame({'center_pixel_id':pix_id, 'sta':sta, 'spl':spl, 'drug_spl':spl_pharma})\n",
    "\n",
    "    #throw out duplicates\n",
    "    data = data.set_index('center_pixel_id')\n",
    "    unique_center_pixels, n_dups = check_for_duplicates(field)\n",
    "    print(\"number of duplicates: \", n_dups)\n",
    "    data = data.loc[data.index.intersection(unique_center_pixels)]\n",
    "    \n",
    "    ipl_mask = (IplMask & field).fetch1('roi_mask_ipl')\n",
    "    coordinates = np.zeros((data.shape[0], 3))\n",
    "    for index, pixel in enumerate(unique_center_pixels):\n",
    "        idx = np.where(ipl_mask==pixel*-1)\n",
    "        coordinates[index, 1:] = np.array([idx[1][0], idx[0][0]]) #z, x\n",
    "        coordinates[index, 0] = pixel\n",
    "\n",
    "    data['z_position']= coordinates[:,1]\n",
    "    data['x_position']=coordinates[:,2]\n",
    "    \n",
    "    #get line of the gcl and inl\n",
    "    left, right, thick = (IplRestrictionInfo & field).fetch1('left','right','thick')\n",
    "    m1, b1 = get_line([(0,left), (ipl_mask.shape[0],right)])\n",
    "\n",
    "    shift = m1*data['x_position']+b1\n",
    "    data['depth']=(data['z_position']-shift)/(thick)\n",
    "    \n",
    "    data['field'] = np.ones(data.shape[0])*num\n",
    "    \n",
    "    #do clustering on the field and assign groups\n",
    "    splines = np.asarray(data['spl'])\n",
    "    spline_array = np.zeros((splines.shape[0], splines[0].shape[0]*splines[0].shape[1]))\n",
    "\n",
    "    for i, spline in enumerate(splines):\n",
    "        current_spline = spline.flatten()\n",
    "        spline_array[i,:] = current_spline\n",
    "        \n",
    "    spline_array_cropped = spline_array[:, 3000:5500]##spline_array[:, 300:-300]\n",
    "    \n",
    "    # clustering\n",
    "    ZSign = cluster.hierarchy.linkage(spline_array_cropped, 'ward')\n",
    "    ListLeavesSign = np.flipud(cluster.hierarchy.leaves_list(ZSign))\n",
    "    TimeTracesSign = np.copy(spline_array)\n",
    "    TimeTracesOrderedSign = TimeTracesSign[ListLeavesSign]\n",
    "    \n",
    "    # flattening\n",
    "    t_value = 0.05\n",
    "    criterion = 'distance' #'maxclust'  #\n",
    "    pixel_assignment = cluster.hierarchy.fcluster(ZSign, t_value, criterion=criterion)\n",
    "    n_clusters = np.unique(pixel_assignment).shape[0]\n",
    "    data['cluster_assignment']=pixel_assignment\n",
    "    print(n_clusters)\n",
    "    \n",
    "    #get cluster averages\n",
    "    clusters = np.unique(np.array(data['cluster_assignment']))\n",
    "\n",
    "    average_filters = np.zeros((data['spl'].iloc[0].shape[0], data['spl'].iloc[0].shape[1], clusters.shape[0]))\n",
    "\n",
    "    for i, clust in enumerate(clusters):\n",
    "        current_frame = data[data['cluster_assignment']==clust]    \n",
    "        current_mean = current_frame['spl'].mean()\n",
    "        average_filters[:,:,i] = current_mean\n",
    "        \n",
    "    #finding centers of cluster averages\n",
    "    max_y = np.zeros(average_filters.shape[2])\n",
    "    max_t = np.zeros(average_filters.shape[2])\n",
    "    mean_df = data.groupby(['cluster_assignment']).mean()\n",
    "    std_df = data.groupby(['cluster_assignment']).std()\n",
    "    for i in range(average_filters.shape[2]):\n",
    "        current_filter = average_filters[50:-30,:, i]\n",
    "        index_max = np.unravel_index(np.argmax(current_filter, axis=None), current_filter.shape)\n",
    "        index_max = np.unravel_index(np.argmax(current_filter, axis=None), current_filter.shape)\n",
    "        index_min = np.unravel_index(np.argmin(current_filter, axis=None), current_filter.shape)\n",
    "        index_min = np.unravel_index(np.argmin(current_filter, axis=None), current_filter.shape)\n",
    "        if mean_df['depth'].iloc[i] > 0.41:\n",
    "            max_y[i] = index_min[1]\n",
    "            max_t[i] = index_min[0]+50\n",
    "        else:\n",
    "            max_y[i] = index_max[1]\n",
    "            max_t[i] = index_max[0]+50\n",
    "    mean_df['max_y'] = max_y\n",
    "    mean_df['max_t'] = max_t\n",
    "    \n",
    "    print(max_y)\n",
    "#     print(max_y)\n",
    "    \n",
    "#     #get ratio of 1st and 2nd svd component to look at S-T separability\n",
    "#     svd_check = np.zeros(average_filters.shape[2])\n",
    "#     for i in range(average_filters.shape[2]):\n",
    "#         u, s, vh = np.linalg.svd(average_filters[50:-30,:,i].T)  # do the svd\n",
    "#         svd_check[i] = s[1]/s[0]\n",
    "    \n",
    "#     mean_df['svd_ratio'] = svd_check\n",
    "    \n",
    "    #set up for realignment using the cluster centers\n",
    "    cluster_center = mean_df['max_y'].loc[data['cluster_assignment']]\n",
    "    data['cluster_center'] = np.array(cluster_center)\n",
    "    \n",
    "    #create the centered spline fits\n",
    "    new_center = 13\n",
    "    max_shift = 6\n",
    "    total_shift = 12\n",
    "    shifts = data['cluster_center']-new_center\n",
    "    round_shifts = np.round(shifts)\n",
    "\n",
    "    shifted_filters = np.zeros((np.int(data['spl'].iloc[0].shape[1]+total_shift), \n",
    "                                data['spl'].iloc[0].shape[0], data.shape[0]))\n",
    "    shifted_filters = np.where(shifted_filters==0, np.nan, shifted_filters)\n",
    "    print(shifted_filters.shape)\n",
    "    shifted_filters_drug = np.zeros((np.int(data['drug_spl'].iloc[0].shape[1]+total_shift), \n",
    "                                data['drug_spl'].iloc[0].shape[0], data.shape[0]))\n",
    "    shifted_filters_drug = np.where(shifted_filters==0, np.nan, shifted_filters)\n",
    "    \n",
    "    data['shifted_filters']=data['spl']\n",
    "    data['shifted_filters_drug']=data['drug_spl']\n",
    "\n",
    "    begin = max_shift\n",
    "    end = max_shift+data['spl'].iloc[0].shape[1]\n",
    "    print(begin, end)\n",
    "    roi_list = data.index\n",
    "    count = 0\n",
    "    for i, roi in enumerate(roi_list):\n",
    "        current_filter = data['spl'].loc[roi]\n",
    "        current_filter = current_filter/np.linalg.norm(current_filter) #normalizing these\n",
    "        shift = round_shifts.loc[roi]\n",
    "        begin_shift = np.int(begin-shift)\n",
    "        end_shift = np.int(end-shift)\n",
    "        \n",
    "        if end_shift > shifted_filters.shape[0]:\n",
    "            print(\"cropped ROI \", roi, \" shift of \", shift)\n",
    "            count+=1\n",
    "            \n",
    "            fixed_dim = shifted_filters.shape[0]-end_shift\n",
    "#             print(fixed_dim)\n",
    "#             print(end_shift, fixed_dim, shifted_filters.shape[0]-1)\n",
    "            shifted_filters[begin_shift:end_shift,:,i] = current_filter.T[:fixed_dim, :]\n",
    "            data['shifted_filters'].loc[roi] = shifted_filters[:,:,i].T\n",
    "        \n",
    "            #also shift the drug splines in the same way\n",
    "            current_filter = data['drug_spl'].loc[roi]\n",
    "            current_filter = current_filter/np.linalg.norm(current_filter) #normalizing these\n",
    "            shifted_filters_drug[begin_shift:end_shift,:,i] = current_filter.T[:fixed_dim, :]\n",
    "            data['shifted_filters_drug'].loc[roi] = shifted_filters_drug[:,:,i].T\n",
    "        else:   \n",
    "            shifted_filters[begin_shift:end_shift,:,i] = current_filter.T\n",
    "            data['shifted_filters'].loc[roi] = shifted_filters[:,:,i].T\n",
    "\n",
    "            #also shift the drug splines in the same way\n",
    "            current_filter = data['drug_spl'].loc[roi]\n",
    "            current_filter = current_filter/np.linalg.norm(current_filter) #normalizing these\n",
    "            shifted_filters_drug[begin_shift:end_shift,:,i] = current_filter.T\n",
    "            data['shifted_filters_drug'].loc[roi] = shifted_filters_drug[:,:,i].T\n",
    "    \n",
    "\n",
    "# mean_shifted_filters = np.nanmean(shifted_filters, axis=2)\n",
    "    \n",
    "    print(\"fixed \", count, \" rois in total\")\n",
    "    return data, TimeTracesOrderedSign, average_filters, mean_df, std_df, shifted_filters, shifted_filters_drug\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_roi_sizes(field):\n",
    "    #get the roi groups and scaling information\n",
    "    roi_groups = (NoiseSplineROIs() & field).fetch('pixel_list')\n",
    "    x_zoom = (Presentation.ScanInfo() & field).fetch1('zoom')\n",
    "    z_zoom = (Presentation.ScanInfo() & field).fetch1('user_zoomz')\n",
    "    roimask = (Field.RoiMask() & field).fetch1('roi_mask')\n",
    "    x_dimension = roimask.shape[0]\n",
    "    z_dimension = roimask.shape[0]\n",
    "    \n",
    "    um_pixel_x = (71.5/x_zoom)/x_dimension\n",
    "    um_pixel_z = (61.6/z_zoom)/z_dimension\n",
    "    um_pixel_both = np.sqrt(um_pixel_x*um_pixel_z)\n",
    "    \n",
    "    #get the diameter of each ROI scaled in microns\n",
    "    roi_sizes = [np.sqrt(len(x))*um_pixel_both for x in roi_groups]\n",
    "    print(np.mean(roi_sizes))\n",
    "    return roi_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s_df1, ordered_traces1, average_filters1, mean_df1, std_df1, shifted_filters1, shifted_filters_drug1 = get_spline_frame_pharma(\n",
    "#     field1_c, field1_s, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s_df2, ordered_traces2, average_filters2, mean_df2, std_df2, shifted_filters2, shifted_filters_drug2 = get_spline_frame_pharma(\n",
    "#     field2_c, field2_s, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_df33, ordered_traces3, average_filters3, mean_df3, std_df3, shifted_filters3, shifted_filters_drug3 = get_spline_frame_pharma(\n",
    "#     field3_c, field3_s, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_df4, ordered_traces4, average_filters4, mean_df4, std_df4, shifted_filters4, shifted_filters_drug4 = get_spline_frame_pharma(\n",
    "#     field4_c, field4_s, 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_df5, ordered_traces5, average_filters5, mean_df5, std_df5, shifted_filters5, shifted_filters_drug5 = get_spline_frame_pharma(\n",
    "#     field5_c, field5_s, 55) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull these from hdf5\n",
    "s_df1 = pd.read_hdf('Data/FigS08.hdf5', 's_df1')\n",
    "s_df2 = pd.read_hdf('Data/FigS08.hdf5', 's_df2')\n",
    "s_df33 = pd.read_hdf('Data/FigS08.hdf5', 's_df33')\n",
    "s_df4 = pd.read_hdf('Data/FigS08.hdf5', 's_df4')\n",
    "s_df5 = pd.read_hdf('Data/FigS08.hdf5', 's_df5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df_xz_new = pd.concat([s_df1, s_df2, s_df33, s_df4, s_df5]) #s_df2, not ready yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattening\n",
    "splines = np.asarray(s_df_xz_new['shifted_filters'])\n",
    "spline_array = np.zeros((splines.shape[0], (splines[0].shape[0])*splines[0].shape[1]))\n",
    "spline_array_cropped = np.zeros((splines.shape[0], 212*7))\n",
    "\n",
    "for i, spline in enumerate(splines):\n",
    "    current_spline = spline\n",
    "    current_spline_flat = current_spline.flatten(order='F')\n",
    "    spline_array[i,:] = current_spline_flat\n",
    "    \n",
    "    current_spline_cropped = spline[30:-70,12:19] #cropping the time differently to deal with the stimulator delay!!\n",
    "    current_spline_cropped_flat = current_spline_cropped.flatten()#order='F'\n",
    "    spline_array_cropped[i,:] = current_spline_cropped_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattening the other way too\n",
    "splines = np.asarray(s_df_xz_new['shifted_filters'])\n",
    "\n",
    "spline_array_orderC = np.zeros((splines.shape[0], (splines[0].shape[0])*splines[0].shape[1]))\n",
    "\n",
    "for i, spline in enumerate(splines):\n",
    "    current_spline = spline\n",
    "    current_spline_flat = current_spline.flatten()#order='F')\n",
    "    spline_array_orderC[i,:] = current_spline_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscored_spline_array = ((spline_array_cropped.T-np.mean(spline_array_cropped, axis=1))/np.std(spline_array_cropped, axis=1)).T\n",
    "\n",
    "threshold = 2.5\n",
    "quality_mask = np.where(np.amax(np.abs(zscored_spline_array), axis=1)>threshold)\n",
    "\n",
    "s_df_xz_new_masked = s_df_xz_new.iloc[quality_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_new = np.copy(spline_array_cropped[quality_mask[0],:])\n",
    "\n",
    "#sparsify the RFs\n",
    "# transformer = SparsePCA(n_components=7, alpha=0.3, random_state=0) #, random_state=0 #(sets random seed)\n",
    "# transformer.fit(x)\n",
    "x_transformed_new = transformer.transform(x_new)\n",
    "x_transformed_new.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_transformed_ipl_new = np.concatenate((x_transformed_new[:,:4], np.expand_dims(s_df_xz_new_masked['depth'], axis=1)), axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.imshow(x_transformed_ipl_new, aspect='auto')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels_new = gm.predict(x_transformed_ipl_new)\n",
    "s_df_xz_new_masked['new_cluster_assignment'] = labels_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull from hdf5 file\n",
    "#read hdf5 file for this notebook\n",
    "f1 = h5py.File(\"Data/FigS08.hdf5\", \"r\")\n",
    "\n",
    "#get the data\n",
    "cluster_assignments = np.array(f1['cluster_assignments'])\n",
    "f1.close()\n",
    "\n",
    "s_df_xz_new_masked['new_cluster_assignment'] = cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df_xz_copy = s_df_xz_new_masked.copy(deep=True)\n",
    "s_df_xz_copy = s_df_xz_copy.set_index(['new_cluster_assignment'])\n",
    "\n",
    "cluster_avg_depth = s_df_xz_copy['depth'].groupby(['new_cluster_assignment']).mean()\n",
    "\n",
    "cluster_avg_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from original clustering\n",
    "ipl_depth_order = np.array([0, 3, 9, 7, 11, 2, 8, 5, 6, 1, 10, 12, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plot clusters by IPL depth\n",
    "n_bins = 10\n",
    "current_palette = sns.color_palette()\n",
    "\n",
    "cmap_name = 'my_list'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, current_palette, N=n_bins)\n",
    "\n",
    "c = s_df_xz_new_masked['field']\n",
    "\n",
    "fig=plt.figure(figsize=(20,5))\n",
    "\n",
    "\n",
    "ax = sns.swarmplot(data=s_df_xz_new_masked, x=\"new_cluster_assignment\", y=\"depth\", hue=\"field\",\n",
    "                   alpha=0.6, cmap='cm', size=2.1, order=ipl_depth_order) #color='k', \n",
    "# plt.scatter(s_df_xz['new_cluster_assignment'], s_df_xz['depth'], alpha=0.3, c=c, cmap=cm)\n",
    "# plt.colorbar()\n",
    "plt.scatter(cluster_avg_depth.index, cluster_avg_depth[ipl_depth_order], color='k')\n",
    "\n",
    "axes = plt.gca()\n",
    "# axes.axhline(on_top, linestyle='dotted', color='k')\n",
    "# axes.axhline(on_bottom, linestyle='dotted', color='k')\n",
    "# axes.axhline(on_mean, linestyle='dotted', color='k')\n",
    "# axes.axhline(off_top, linestyle='dotted', color='k')\n",
    "# axes.axhline(off_bottom, linestyle='dotted', color='k')\n",
    "# axes.axhline(off_mean, linestyle='dotted', color='k')\n",
    "axes.set_xlabel('cluster #')\n",
    "axes.set_ylabel('IPL depth')\n",
    "plt.grid(True)\n",
    "\n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Clusters_ipl_depth_chat.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert the cluster assignments into the database"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cluster_info = dict(\n",
    "            spline_cluster_params_id = 4,\n",
    "            group_name = 'ubiquitous_iglusnfr_xz_pharma',\n",
    "            n_clusters = 13,\n",
    "            random_state = 0,\n",
    "    )\n",
    "\n",
    "NoiseSplineClusterParams.insert1(cluster_info)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#list of field keys from above\n",
    "field_list_all_xz = [field1_c, field2_c, field3_c, field4_c, field5_c]#[field3, field6, field7, field13]\n",
    "field_nums_xz = [11, 22, 33, 44, 55]#[3, 6, 7, 13]\n",
    "dataframe_with_clusters = s_df_xz_new_masked\n",
    "\n",
    "#insert and named group of fields in cell above\n",
    "spline_cluster_params_id = 4\n",
    "\n",
    "#go through each field, pull the full primary key for the splines, add the cluster assignment and insert\n",
    "for i, field in enumerate(field_list_all_xz):\n",
    "    roi_ids_to_get = np.array(dataframe_with_clusters[dataframe_with_clusters['field']==field_nums_xz[i]].index)\n",
    "    list_of_conds = ['center_pixel_id = '+str(x) for x in roi_ids_to_get]\n",
    "    keys_to_insert = (NoiseSplineRF2() & field & list_of_conds).fetch(\"KEY\")\n",
    "    cluster_assignments = np.array(dataframe_with_clusters[dataframe_with_clusters['field']==field_nums_xz[i]]['new_cluster_assignment'])\n",
    "    [(dicty.update({'cluster_assignment': cluster_assignments[j], 'spline_cluster_params_id': spline_cluster_params_id})) for j, dicty in enumerate(keys_to_insert)]\n",
    "\n",
    "    \n",
    "    NoiseSplineClusterAssignment.insert(keys_to_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make combined df for doing the averages\n",
    "s_df_xz_new_masked['shifted_filters_corrected'] = s_df_xz_new_masked['shifted_filters'].apply(lambda x: x[:-40,:])\n",
    "s_df_xz_new_masked['shifted_filters_corrected_drug'] = s_df_xz_new_masked['shifted_filters_drug'].apply(lambda x: x[:-40,:])\n",
    "\n",
    "s_df_xz_masked['shifted_filters_corrected'] = s_df_xz_masked['shifted_filters'].apply(lambda x: x[40:,:])\n",
    "\n",
    "allroi_df = pd.concat([s_df_xz_new_masked, s_df_xz_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make average filters\n",
    "clusters = np.unique(np.array(allroi_df['new_cluster_assignment']))\n",
    "\n",
    "average_filters = np.zeros((allroi_df['shifted_filters_corrected'].iloc[0].shape[0], allroi_df['shifted_filters_corrected'].iloc[0].shape[1], clusters.shape[0]))\n",
    "average_filters_drug = np.zeros((s_df_xz_new_masked['shifted_filters_corrected'].iloc[0].shape[0], s_df_xz_new_masked['shifted_filters_corrected'].iloc[0].shape[1], clusters.shape[0]))\n",
    "\n",
    "for i, clust in enumerate(clusters):\n",
    "    current_frame = allroi_df[allroi_df['new_cluster_assignment']==clust]    \n",
    "    current_mean = np.nanmean(np.dstack(current_frame['shifted_filters_corrected']), axis=2)\n",
    "    average_filters[:,:,i] = current_mean\n",
    "    \n",
    "    current_frame = s_df_xz_new_masked[s_df_xz_new_masked['new_cluster_assignment']==clust]\n",
    "    current_mean = np.nanmean(np.dstack(current_frame['shifted_filters_corrected_drug']), axis=2)\n",
    "    average_filters_drug[:,:,i] = current_mean\n",
    "    \n",
    "\n",
    "#make SD of filters\n",
    "clusters = np.unique(np.array(allroi_df['new_cluster_assignment']))\n",
    "\n",
    "sd_filters = np.zeros((allroi_df['shifted_filters_corrected'].iloc[0].shape[0], allroi_df['shifted_filters_corrected'].iloc[0].shape[1], clusters.shape[0]))\n",
    "sd_filters_drug = np.zeros((\n",
    "    s_df_xz_new_masked['shifted_filters_corrected'].iloc[0].shape[0], s_df_xz_new_masked['shifted_filters_corrected'].iloc[0].shape[1], clusters.shape[0]))\n",
    "\n",
    "\n",
    "for i, clust in enumerate(clusters):\n",
    "    current_frame = allroi_df[allroi_df['new_cluster_assignment']==clust]    \n",
    "    current_sd = np.nanstd(np.dstack(current_frame['shifted_filters_corrected']), axis=2)\n",
    "    sd_filters[:,:,i] = current_sd\n",
    "    \n",
    "    current_frame = s_df_xz_new_masked[s_df_xz_new_masked['new_cluster_assignment']==clust]\n",
    "    current_sd = np.nanstd(np.dstack(current_frame['shifted_filters_corrected_drug']), axis=2)\n",
    "    sd_filters_drug[:,:,i] = current_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#plot the cluster averages and normalized temporal kernels and calculate latency and surround strength\n",
    "\n",
    "polarity = [1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1]\n",
    "latency = np.zeros(len(polarity))\n",
    "surround_strength = np.zeros(len(polarity))\n",
    "\n",
    "latency_drug = np.zeros(len(polarity))\n",
    "surround_strength_drug = np.zeros(len(polarity))\n",
    "\n",
    "center_change_index = np.zeros(len(polarity))\n",
    "surround_change_index =  np.zeros(len(polarity))\n",
    "\n",
    "maxmin = 0.06\n",
    "plot_range = 2\n",
    "\n",
    "fig, ax = plt.subplots(6, 13, figsize=(20, 10))\n",
    "for i, clust in enumerate(ipl_depth_order):\n",
    "    \n",
    "#     ax[0, i].imshow(spline_array[quality_mask[0],:][s_df_xz_masked['new_cluster_assignment']==clust], aspect=12, cmap='Greys_r')\n",
    "#     ax[0, i].set_yticklabels([])\n",
    "#     ax[0, i].set_xticklabels([])\n",
    "#     ax[0, i].axis('off')\n",
    "    \n",
    "    ax[0, i].imshow(average_filters[:,10:26,clust]/np.linalg.norm(average_filters[:,11:19,clust]), \n",
    "                    aspect='auto', cmap='Greys_r', vmin=-maxmin, vmax=maxmin)\n",
    "    if i == 0:\n",
    "        ax[0, i].axvline(10-10, color='k', linestyle='dotted')\n",
    "        ax[0, i].axvline(16-10, color='k', linestyle='dotted')\n",
    "        ax[0, i].axvline(18-10, color='r', linestyle='dotted')\n",
    "        ax[0, i].axvline(20-10, color='r', linestyle='dotted')\n",
    "    ax[0, i].axis('off')\n",
    "    \n",
    "    ax[1, i].imshow(average_filters_drug[:,10:26,clust]/np.linalg.norm(average_filters[:,11:19,clust]), \n",
    "                    aspect='auto', cmap='Greys_r', vmin=-maxmin, vmax=maxmin)\n",
    "    \n",
    "    ax[1, i].axis('off')\n",
    "\n",
    "    \n",
    "    \n",
    "    avg_filter_flipped = average_filters[:,:,clust]*polarity[clust] #deal with off cells\n",
    "\n",
    "    ax[2, i].plot(polarity[clust]*np.mean(avg_filter_flipped[:,18:21], axis=1)/np.amax(np.mean(avg_filter_flipped[50:,18:21], axis=1)), color='k',)\n",
    "    ax[2, i].plot(polarity[clust]*np.mean(avg_filter_flipped[:,10:17], axis=1)/np.amin(np.mean(avg_filter_flipped[100:,10:17], axis=1))*-1, color='grey')\n",
    "#     ax[2].plot(np.mean(avg_filter_flipped[:,22:26], axis=1)/np.amin(np.mean(avg_filter_flipped[100:290,22:26], axis=1))*-1, color='b')\n",
    "    ax[2, i].axhline(color='k', linestyle='dotted')\n",
    "    ax[2, i].set_yticklabels([])\n",
    "    ax[2, i].set_xticklabels([])\n",
    "    ax[2, i].axis('off')\n",
    "    ax[2, i].set_ylim([-plot_range,plot_range])\n",
    "    \n",
    "    \n",
    "    avg_filter_flipped_drug = average_filters_drug[:,:,clust]*polarity[clust] #deal with off cells\n",
    "\n",
    "    ax[3, i].plot(polarity[clust]*np.mean(avg_filter_flipped_drug[:,18:21], axis=1)/np.amax(np.mean(avg_filter_flipped[50:,18:21], axis=1)), color='k',)\n",
    "    ax[3, i].plot(polarity[clust]*np.mean(avg_filter_flipped_drug[:,10:17], axis=1)/np.amin(np.mean(avg_filter_flipped[100:,10:17], axis=1))*-1, color='grey')\n",
    "#     ax[2].plot(np.mean(avg_filter_flipped[:,22:26], axis=1)/np.amin(np.mean(avg_filter_flipped[100:290,22:26], axis=1))*-1, color='b')\n",
    "    ax[3, i].axhline(color='k', linestyle='dotted')\n",
    "    ax[3, i].set_yticklabels([])\n",
    "    ax[3, i].set_xticklabels([])\n",
    "    ax[3, i].axis('off')\n",
    "    ax[3, i].set_ylim([-plot_range,plot_range])\n",
    "    \n",
    "    \n",
    "    ax[4, i].plot(polarity[clust]*np.mean(avg_filter_flipped[:,18:21], axis=1)/np.amax(np.mean(avg_filter_flipped[50:,18:21], axis=1)), color='k',)\n",
    "    ax[4, i].plot(polarity[clust]*np.mean(avg_filter_flipped_drug[:,18:21], axis=1)/np.amax(np.mean(avg_filter_flipped[50:,18:21], axis=1)), color='r',)\n",
    "#     ax[2].plot(np.mean(avg_filter_flipped[:,22:26], axis=1)/np.amin(np.mean(avg_filter_flipped[100:290,22:26], axis=1))*-1, color='b')\n",
    "    ax[4, i].axhline(color='k', linestyle='dotted')\n",
    "    ax[4, i].set_yticklabels([])\n",
    "    ax[4, i].set_xticklabels([])\n",
    "    ax[4, i].axis('off')\n",
    "    ax[4, i].set_ylim([-plot_range,plot_range])\n",
    "    \n",
    "    ax[5, i].plot(polarity[clust]*np.mean(avg_filter_flipped[:,10:17], axis=1)/np.amin(np.mean(avg_filter_flipped[100:,10:17], axis=1))*-1, color='k')\n",
    "    ax[5, i].plot(polarity[clust]*np.mean(avg_filter_flipped_drug[:,10:17], axis=1)/np.amin(np.mean(avg_filter_flipped[100:,10:17], axis=1))*-1, color='r')\n",
    "#     ax[2].plot(np.mean(avg_filter_flipped[:,22:26], axis=1)/np.amin(np.mean(avg_filter_flipped[100:290,22:26], axis=1))*-1, color='b')\n",
    "    ax[5, i].axhline(color='k', linestyle='dotted')\n",
    "    ax[5, i].set_yticklabels([])\n",
    "    ax[5, i].set_xticklabels([])\n",
    "    ax[5, i].axis('off')\n",
    "    ax[5, i].set_ylim([-plot_range,plot_range])\n",
    "    \n",
    "#     Figpath = 'Fig_temp/'\n",
    "#     savename = Figpath+\"Cluster_avgs_\"+str(i)+\".pdf\"\n",
    "#     plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "\n",
    "#     plt.show()\n",
    "    center = np.mean(avg_filter_flipped[100:230,18:21], axis=1)/np.amax(np.mean(avg_filter_flipped[100:230,18:21], axis=1))\n",
    "    surround = np.mean(avg_filter_flipped[100:230,10:17], axis=1)/np.amin(np.mean(avg_filter_flipped[100:230,10:17], axis=1))\n",
    "    \n",
    "    center_drug = np.mean(avg_filter_flipped_drug[100:230,18:21], axis=1)/np.amax(np.mean(avg_filter_flipped[100:230,18:21], axis=1))\n",
    "    surround_drug = np.mean(avg_filter_flipped_drug[100:230,10:17], axis=1)/np.amin(np.mean(avg_filter_flipped[100:230,10:17], axis=1))\n",
    "    \n",
    "    latency[clust] = np.argmax(center)-np.argmax(surround) \n",
    "    surround_strength[clust] = np.amin(np.mean(avg_filter_flipped[50:230,10:17], axis=1))/np.amax(np.mean(avg_filter_flipped[50:230,18:21], axis=1))\n",
    "\n",
    "    latency_drug[clust] = np.argmax(center_drug)-np.argmax(surround_drug) \n",
    "    surround_strength_drug[clust] = np.amin(np.mean(avg_filter_flipped_drug[50:230,10:17], axis=1))/np.amax(np.mean(avg_filter_flipped_drug[50:230,18:21], axis=1))\n",
    "\n",
    "    center_change_index[clust] = (np.amax(center) - np.amax(center_drug))/np.amax(center)\n",
    "    surround_change_index[clust] = (np.amax(surround) - np.amax(surround_drug))/np.amax(surround)\n",
    "    \n",
    "    \n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Cluster_avgs_strychnine.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gauss(x, H, A, x0, sigma):\n",
    "    return H + A * np.exp(-(x - x0) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "def gauss_fit(x, y):\n",
    "    mean = sum(x * y) / sum(y)\n",
    "    sigma = np.sqrt(sum(y * (x - mean) ** 2) / sum(y))\n",
    "    popt, pcov = curve_fit(gauss, x, y, p0=[min(y), max(y), mean, sigma])\n",
    "    return popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate center FWHM by doing Gaussian Fit\n",
    "\n",
    "center_fwhm=np.zeros(clusters.shape[0])\n",
    "colors_polarity = ['black', 'red', 'blue']\n",
    "for i in range(clusters.shape[0]):\n",
    "    mean_filter = np.mean(average_filters[200:220,13:26,i], axis=0)*polarity[i]\n",
    "    space_positions = np.arange(mean_filter.shape[0])*20 #in microns\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(space_positions, mean_filter/np.amax(mean_filter)+1, color=colors_polarity[polarity[i]+1])\n",
    "    \n",
    "\n",
    "    \n",
    "    H, A, x0, sigma = gauss_fit(space_positions, mean_filter/np.amax(mean_filter)+1) #adding one to get the algorithm to behave\n",
    "    FWHM = 2.35482 * sigma\n",
    "    center_fwhm[i] = FWHM\n",
    "    print(i, FWHM)\n",
    "    plt.plot(space_positions, gauss(space_positions, *gauss_fit(space_positions, mean_filter/np.amax(mean_filter)+1)), '--r', label='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate center FWHM by doing Gaussian Fit\n",
    "\n",
    "center_fwhm_drug=np.zeros(clusters.shape[0])\n",
    "colors_polarity = ['black', 'red', 'blue']\n",
    "for i in range(clusters.shape[0]):\n",
    "    mean_filter = np.mean(average_filters_drug[200:220,13:26,i], axis=0)*polarity[i] #correcting for stimulator delay\n",
    "    space_positions = np.arange(mean_filter.shape[0])*20 #in microns\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(space_positions, mean_filter/np.amax(mean_filter)+1, color=colors_polarity[polarity[i]+1])\n",
    "    \n",
    "\n",
    "    \n",
    "    H, A, x0, sigma = gauss_fit(space_positions, mean_filter/np.amax(mean_filter)+1) #adding one to get the algorithm to behave\n",
    "    FWHM = 2.35482 * sigma\n",
    "    center_fwhm_drug[i] = FWHM\n",
    "    \n",
    "    plt.plot(space_positions, gauss(space_positions, *gauss_fit(space_positions, mean_filter/np.amax(mean_filter)+1)), '--r', label='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate biphasic index\n",
    "\n",
    "center_biphasic_index=np.zeros(clusters.shape[0])\n",
    "\n",
    "for i in range(clusters.shape[0]):\n",
    "    mean_filter = np.mean(average_filters[:,18:21,i], axis=1)*polarity[i]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(mean_filter)\n",
    "    \n",
    "    filter_max = np.amax(mean_filter)\n",
    "    filter_min = np.amin(mean_filter[50:200])\n",
    "    biphasic_index = filter_min/filter_max*-1\n",
    "    center_biphasic_index[i] = biphasic_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate biphasic index\n",
    "\n",
    "center_biphasic_index_drug=np.zeros(clusters.shape[0])\n",
    "\n",
    "for i in range(clusters.shape[0]):\n",
    "    mean_filter = np.mean(average_filters_drug[:,18:21,i], axis=1)*polarity[i]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(mean_filter)\n",
    "    \n",
    "    filter_max = np.amax(mean_filter)\n",
    "    filter_min = np.amin(mean_filter[50:200])\n",
    "    biphasic_index = filter_min/filter_max*-1\n",
    "    center_biphasic_index_drug[i] = biphasic_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    plt.plot(['ctrl','TPMPA'], [latency[i], latency_drug[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    plt.plot(['ctrl','Strychnine'], [surround_strength[i]*-1, surround_strength_drug[i]*-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    plt.plot(['ctrl','Strychnine'], [abs(center_fwhm[i]), abs(center_fwhm_drug[i])], c='k')\n",
    "    \n",
    "axes=plt.gca()\n",
    "axes.set_xlim([-.15,1.15])\n",
    "    \n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"FWHM_control_strychnine.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    plt.plot(['ctrl','Strychnine'], [center_biphasic_index[i], center_biphasic_index_drug[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(center_change_index, surround_change_index)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-1,1])\n",
    "axes.set_xlim([-1,1])\n",
    "axes.axhline(0)\n",
    "axes.axvline(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_svg_parameters_on_screen(dendrite_length, moving_bar_length, stimulus_velocity,\n",
    "                             starting_point, distance_covered, real_time_lag, time_points, space_per_pixel):\n",
    "    temporal_conversion = time_points / real_time_lag  # From seconds to pixel.\n",
    "    spatial_conversion = 1 / space_per_pixel  # From um to pixel.\n",
    "\n",
    "    # Moving bar is already fully on screen at t = 0\n",
    "    distance_moved = distance_covered - moving_bar_length  # [um]\n",
    "    stimulation_time = (distance_moved / stimulus_velocity)  # [s]\n",
    "\n",
    "    # Convert from seconds & um to pixel\n",
    "    stimulation_time_pixel = stimulation_time * temporal_conversion\n",
    "\n",
    "    distance_moved_pixel = distance_moved * spatial_conversion\n",
    "    dendrite_length_pixel = dendrite_length * spatial_conversion\n",
    "    moving_bar_length_pixel = moving_bar_length * spatial_conversion\n",
    "    starting_point_pixel = starting_point * spatial_conversion\n",
    "    distance_covered_pixel = distance_covered * spatial_conversion\n",
    "\n",
    "    # Slope of diagonal\n",
    "    slope = -stimulation_time_pixel / distance_moved_pixel\n",
    "\n",
    "    # Calculate thickness of diagonal\n",
    "    alpha = np.arctan(stimulation_time_pixel / distance_moved_pixel)\n",
    "    diagonal_thickness = moving_bar_length_pixel * np.sin(alpha)\n",
    "\n",
    "    # Calculate coordinates for beginning & end of diagonal\n",
    "    # Diagonal starts and ends out of picture to allow for a smooth & exact picture.\n",
    "    x_start = starting_point_pixel\n",
    "    x_stop = x_start + distance_covered_pixel\n",
    "\n",
    "    y_start = (moving_bar_length_pixel / 2) * slope  # Negative\n",
    "    y_stop = stimulation_time_pixel - y_start\n",
    "\n",
    "    return (stimulation_time_pixel, dendrite_length_pixel,\n",
    "            diagonal_thickness,\n",
    "            x_start, x_stop, y_start, y_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull the convolution parameters for modeling\n",
    "convolution_params = {}\n",
    "file = h5py.File('Data/Fig5.hdf5', 'r')\n",
    "dict_group_load = file['convolution_params']\n",
    "dict_group_keys = dict_group_load.keys()\n",
    "for k in dict_group_keys:\n",
    "    convolution_params[k]= np.float(dict_group_load[k][0])\n",
    "    \n",
    "real_time_lag = convolution_params['real_time_lag']\n",
    "time_points = convolution_params['time_points']\n",
    "space_per_pixelx = convolution_params['space_per_pixelx']\n",
    "space_per_pixely = convolution_params['space_per_pixely']\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get response predictions for 1000 um/s for display\n",
    "\n",
    "# get the stimulus for convolution\n",
    "\n",
    "convolution_params['stimulus_velocity'] = 1000 # WARNING altering this to play around with velocity tuning\n",
    "convolution_params['dendrite_length'] = 140\n",
    "convolution_params['distance_covered'] = 140\n",
    "if space_per_pixelx < space_per_pixely:  # check which is the shorter dimension of the stimulus\n",
    "    space_per_pixel = space_per_pixelx\n",
    "else:\n",
    "    space_per_pixel = space_per_pixely\n",
    "\n",
    "time, dend, diag, x_start, x_stop, y_start, y_stop = get_svg_parameters_on_screen(\n",
    "    convolution_params['dendrite_length'],\n",
    "    convolution_params['moving_bar_length'],\n",
    "    convolution_params['stimulus_velocity'],\n",
    "    convolution_params['starting_point'],\n",
    "    convolution_params['distance_covered'],\n",
    "    real_time_lag,\n",
    "    time_points,\n",
    "    space_per_pixel\n",
    ")\n",
    "\n",
    "string_svg = \"\"\"<svg width=\"{0}\" height=\"{1}\" viewBox=\"0 0 {2} {3}\">\n",
    "    <rect x=\"0\" y=\"0\" width=\"{4}\" height=\"{5}\" fill=\"black\" />\n",
    "      <line x1=\"{6}\" y1=\"{7}\" x2=\"{8}\" y2=\"{9}\"\n",
    "          stroke-width=\"{10}\" stroke=\"white\" stroke-linecap=\"square\"/>\n",
    "    </svg>\"\"\".format(dend, time, dend, time, dend, time, x_start, y_start, x_stop, y_stop, diag)\n",
    "\n",
    "folder_str = 'Data/'\n",
    "file_name = str(1)\n",
    "name_svg = folder_str + file_name + \".svg\"\n",
    "name_png = folder_str + file_name + \".png\"\n",
    "\n",
    "new_file = open(name_svg, \"wt\")\n",
    "new_file.write(string_svg)\n",
    "new_file.close()\n",
    "\n",
    "cairosvg.svg2png(url=name_svg, write_to=name_png)\n",
    "\n",
    "stimulus_image = np.array(Image.open(name_png).convert('L'))\n",
    "\n",
    "#get convolved responses\n",
    "\n",
    "# cluster_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,]\n",
    "dsi = np.zeros(13)\n",
    "dsi_off = np.zeros(13)\n",
    "\n",
    "fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "for i, clust in enumerate(ipl_depth_order):\n",
    "    # get the necessaries\n",
    "    # spline = (NoiseSplineRF2 & key).fetch1('spl')\n",
    "    spline = average_filters[:,:,clust]\n",
    "    edge_buffer = 1 #changing to at least partially correct for the \n",
    "    stimulus_image_rev = np.flip(stimulus_image, axis=0)\n",
    "    kernel_type = 'left'\n",
    "    \n",
    "    # set up and crop the kernel\n",
    "    kernel_length = stimulus_image.shape[1]  # needs to be the same length as the stimulus\n",
    "\n",
    "    # get the location of the RF center in the kernel\n",
    "    spline_temp = spline[edge_buffer:-(edge_buffer+20), :] #correcting for time offset\n",
    "    index_max = np.unravel_index(np.argmax(np.abs(spline_temp), axis=None), spline_temp.shape)\n",
    "    peak_y = index_max[1]\n",
    "    peak_y = 19\n",
    "\n",
    "    # crop the kernel depending on the type of experiment\n",
    "    if kernel_type == 'right':\n",
    "        spline_cut = spline[edge_buffer:-(edge_buffer+20), np.int(peak_y - 1):np.int(peak_y + kernel_length - 1)]\n",
    "    elif kernel_type == 'left':\n",
    "        spline_cut = spline[edge_buffer:-(edge_buffer+20), np.int(peak_y + 1 - kernel_length):np.int(peak_y + 1)]\n",
    "    elif kernel_type == 'full':\n",
    "        spline_cut = spline[edge_buffer:-(edge_buffer+20),\n",
    "                     0:kernel_length]  # should be full kernel, just adding space range just in case\n",
    "    elif kernel_type == 'centered':\n",
    "        spline_cut = spline[edge_buffer:-(edge_buffer+20),\n",
    "                     np.int(np.floor(peak_y - kernel_length / 2)):np.int(np.floor(peak_y + kernel_length / 2))]\n",
    "    else:\n",
    "        spline_cut = spline  # just to make the code happy, this should never be the option\n",
    "    # can code in other possibilities here...\n",
    "\n",
    "    # check whether cut spline is big enough\n",
    "    space_length = spline_cut.shape[1]\n",
    "    if kernel_length == space_length:\n",
    "        empty_flag = 0\n",
    "        # setup output variables\n",
    "        convolved_response = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "        convolved_response_rev = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "\n",
    "        # loop through x positions and convolve\n",
    "        for x_position in range(stimulus_image.shape[1]):\n",
    "            ker = np.flip(spline_cut[:, x_position])\n",
    "            img = stimulus_image[:, x_position]\n",
    "            img2 = stimulus_image_rev[:, x_position]\n",
    "            # and then convolve these two\n",
    "            convolved_response[:, x_position] = np.convolve(a=img, v=ker, mode='full')\n",
    "            convolved_response_rev[:, x_position] = np.convolve(a=img2, v=ker, mode='full')\n",
    "    else:\n",
    "        empty_flag = 1\n",
    "        convolved_response = []\n",
    "        convolved_response_rev = []\n",
    "\n",
    "    response = np.sum(convolved_response, axis=1)\n",
    "    response_rev = np.sum(convolved_response_rev, axis=1)\n",
    "    max_response = np.amax(response[:300])\n",
    "    max_response_rev = np.amax(response_rev[:300])\n",
    "    max_response_off = np.amin(response[:300])\n",
    "    max_response_rev_off = np.amin(response_rev[:300])\n",
    "    dsi[clust] = (max_response - max_response_rev) / (max_response + max_response_rev)\n",
    "    dsi_off[clust] = (max_response_off - max_response_rev_off) / (max_response_off + max_response_rev_off)\n",
    "#     print(dsi, dsi_off)\n",
    "#     plt.figure()\n",
    "    ax[i].plot(response*polarity[clust], color='b')\n",
    "    ax[i].plot(response_rev*polarity[clust], color='k')\n",
    "    ax[i].axhline(color='k', linestyle='dotted')\n",
    "    ax[i].set_yticklabels([])\n",
    "#     ax[i].set_xticklabels([])\n",
    "#     ax[i].axis('off')\n",
    "\n",
    "\n",
    "dsi_all = np.where(cluster_avg_depth<0.5, dsi, dsi_off)\n",
    "dsi_all\n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Convolved_responses_1000um_s_half.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get response predictions for 1000 um/s for display drug\n",
    "\n",
    "# get the stimulus for convolution\n",
    "\n",
    "convolution_params['stimulus_velocity'] = 1000 # WARNING altering this to play around with velocity tuning\n",
    "convolution_params['dendrite_length'] = 140\n",
    "convolution_params['distance_covered'] = 140\n",
    "\n",
    "if space_per_pixelx < space_per_pixely:  # check which is the shorter dimension of the stimulus\n",
    "    space_per_pixel = space_per_pixelx\n",
    "else:\n",
    "    space_per_pixel = space_per_pixely\n",
    "\n",
    "time, dend, diag, x_start, x_stop, y_start, y_stop = get_svg_parameters_on_screen(\n",
    "    convolution_params['dendrite_length'],\n",
    "    convolution_params['moving_bar_length'],\n",
    "    convolution_params['stimulus_velocity'],\n",
    "    convolution_params['starting_point'],\n",
    "    convolution_params['distance_covered'],\n",
    "    real_time_lag,\n",
    "    time_points,\n",
    "    space_per_pixel\n",
    ")\n",
    "\n",
    "string_svg = \"\"\"<svg width=\"{0}\" height=\"{1}\" viewBox=\"0 0 {2} {3}\">\n",
    "    <rect x=\"0\" y=\"0\" width=\"{4}\" height=\"{5}\" fill=\"black\" />\n",
    "      <line x1=\"{6}\" y1=\"{7}\" x2=\"{8}\" y2=\"{9}\"\n",
    "          stroke-width=\"{10}\" stroke=\"white\" stroke-linecap=\"square\"/>\n",
    "    </svg>\"\"\".format(dend, time, dend, time, dend, time, x_start, y_start, x_stop, y_stop, diag)\n",
    "\n",
    "folder_str = 'Data/'\n",
    "file_name = str(1)\n",
    "name_svg = folder_str + file_name + \".svg\"\n",
    "name_png = folder_str + file_name + \".png\"\n",
    "\n",
    "new_file = open(name_svg, \"wt\")\n",
    "new_file.write(string_svg)\n",
    "new_file.close()\n",
    "\n",
    "cairosvg.svg2png(url=name_svg, write_to=name_png)\n",
    "\n",
    "stimulus_image = np.array(Image.open(name_png).convert('L'))\n",
    "\n",
    "\n",
    "\n",
    "#get convolved responses\n",
    "\n",
    "# cluster_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,]\n",
    "dsi = np.zeros(13)\n",
    "dsi_off = np.zeros(13)\n",
    "\n",
    "fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "for i, clust in enumerate(ipl_depth_order):\n",
    "    # get the necessaries\n",
    "    # spline = (NoiseSplineRF2 & key).fetch1('spl')\n",
    "    spline = average_filters_drug[:,:,clust]\n",
    "    edge_buffer = 1 #\n",
    "    stimulus_image_rev = np.flip(stimulus_image, axis=0)\n",
    "    kernel_type = 'left'\n",
    "\n",
    "    # set up and crop the kernel\n",
    "    kernel_length = stimulus_image.shape[1]  # needs to be the same length as the stimulus\n",
    "\n",
    "    # get the location of the RF center in the kernel\n",
    "    spline_temp = spline[edge_buffer:-(edge_buffer+20), :]\n",
    "    index_max = np.unravel_index(np.argmax(np.abs(spline_temp), axis=None), spline_temp.shape)\n",
    "    peak_y = index_max[1]\n",
    "    peak_y = 19\n",
    "\n",
    "    # crop the kernel depending on the type of experiment\n",
    "    if kernel_type == 'right':\n",
    "        spline_cut = spline[edge_buffer:-(edge_buffer+20), np.int(peak_y - 1):np.int(peak_y + kernel_length - 1)]\n",
    "    elif kernel_type == 'left':\n",
    "        spline_cut = spline[edge_buffer:-(edge_buffer+20), np.int(peak_y + 1 - kernel_length):np.int(peak_y + 1)]\n",
    "    elif kernel_type == 'full':\n",
    "        spline_cut = spline[edge_buffer:-(edge_buffer+20),\n",
    "                     0:kernel_length]  # should be full kernel, just adding space range just in case\n",
    "    elif kernel_type == 'centered':\n",
    "        spline_cut = spline[edge_buffer:-(edge_buffer+20),\n",
    "                     np.int(np.floor(peak_y - kernel_length / 2)):np.int(np.floor(peak_y + kernel_length / 2))]\n",
    "    else:\n",
    "        spline_cut = spline  # just to make the code happy, this should never be the option\n",
    "    # can code in other possibilities here...\n",
    "\n",
    "    # check whether cut spline is big enough\n",
    "    space_length = spline_cut.shape[1]\n",
    "    if kernel_length == space_length:\n",
    "        empty_flag = 0\n",
    "        # setup output variables\n",
    "        convolved_response = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "        convolved_response_rev = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "\n",
    "        # loop through x positions and convolve\n",
    "        for x_position in range(stimulus_image.shape[1]):\n",
    "            ker = np.flip(spline_cut[:, x_position])\n",
    "            img = stimulus_image[:, x_position]\n",
    "            img2 = stimulus_image_rev[:, x_position]\n",
    "            # and then convolve these two\n",
    "            convolved_response[:, x_position] = np.convolve(a=img, v=ker, mode='full')\n",
    "            convolved_response_rev[:, x_position] = np.convolve(a=img2, v=ker, mode='full')\n",
    "    else:\n",
    "        empty_flag = 1\n",
    "        convolved_response = []\n",
    "        convolved_response_rev = []\n",
    "\n",
    "    response = np.sum(convolved_response, axis=1)\n",
    "    response_rev = np.sum(convolved_response_rev, axis=1)\n",
    "    max_response = np.amax(response[:300])\n",
    "    max_response_rev = np.amax(response_rev[:300])\n",
    "    max_response_off = np.amin(response[:300])\n",
    "    max_response_rev_off = np.amin(response_rev[:300])\n",
    "    dsi[clust] = (max_response - max_response_rev) / (max_response + max_response_rev)\n",
    "    dsi_off[clust] = (max_response_off - max_response_rev_off) / (max_response_off + max_response_rev_off)\n",
    "#     print(dsi, dsi_off)\n",
    "#     plt.figure()\n",
    "    ax[i].plot(response*polarity[clust], color='b')\n",
    "    ax[i].plot(response_rev*polarity[clust], color='k')\n",
    "    ax[i].axhline(color='k', linestyle='dotted')\n",
    "    ax[i].set_yticklabels([])\n",
    "#     ax[i].set_xticklabels([])\n",
    "#     ax[i].axis('off')\n",
    "\n",
    "\n",
    "dsi_all = np.where(cluster_avg_depth<0.5, dsi, dsi_off)\n",
    "dsi_all\n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Convolved_responses_1000um_s_half.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi_control = np.array([-0.03443281, -0.00679397, -0.14377462, -0.05103353,  0.01832009,\n",
    "        0.00695351, -0.03237312, -0.18226003, -0.27477748, -0.1972912 ,\n",
    "       -0.0391588 , -0.18723202, -0.10174225])*-1\n",
    "dsi_drug = np.array([-0.09827108, -0.02291052, -0.1352631 , -0.00183421, -0.14403772,\n",
    "        0.09831501, -0.02989986, -0.11819533, -0.09414848, -0.13973124,\n",
    "       -0.01065961, -0.13226346, -0.01778581])*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def convolved_dsis(velocity, convolution_params):\n",
    "    # velocity tuning\n",
    "\n",
    "    # get the stimulus for convolution\n",
    "\n",
    "#     convolution_stim_params_id = 'convolution_stim_params_id = 1'\n",
    "#     field = field3\n",
    "\n",
    "#     convolution_params = (ConvolutionStimulusParams() & convolution_stim_params_id).fetch1()\n",
    "    convolution_params['stimulus_velocity'] = velocity # WARNING altering this to play around with velocity tuning\n",
    "    convolution_params['dendrite_length'] = 140\n",
    "    convolution_params['distance_covered'] = 140\n",
    "#     real_time_lag, time_points = (NoiseDesignMatrix2() & field).fetch1('real_time_lag', 'time_points')\n",
    "#     space_per_pixelx, space_per_pixely = (StimulusParams.NoiseParams() & field).fetch1('box_dx_um', 'box_dy_um')\n",
    "    if space_per_pixelx < space_per_pixely:  # check which is the shorter dimension of the stimulus\n",
    "        space_per_pixel = space_per_pixelx\n",
    "    else:\n",
    "        space_per_pixel = space_per_pixely\n",
    "\n",
    "    time, dend, diag, x_start, x_stop, y_start, y_stop = get_svg_parameters_on_screen(\n",
    "        convolution_params['dendrite_length'],\n",
    "        convolution_params['moving_bar_length'],\n",
    "        convolution_params['stimulus_velocity'],\n",
    "        convolution_params['starting_point'],\n",
    "        convolution_params['distance_covered'],\n",
    "        real_time_lag,\n",
    "        time_points,\n",
    "        space_per_pixel\n",
    "    )\n",
    "\n",
    "    string_svg = \"\"\"<svg width=\"{0}\" height=\"{1}\" viewBox=\"0 0 {2} {3}\">\n",
    "        <rect x=\"0\" y=\"0\" width=\"{4}\" height=\"{5}\" fill=\"black\" />\n",
    "          <line x1=\"{6}\" y1=\"{7}\" x2=\"{8}\" y2=\"{9}\"\n",
    "              stroke-width=\"{10}\" stroke=\"white\" stroke-linecap=\"square\"/>\n",
    "        </svg>\"\"\".format(dend, time, dend, time, dend, time, x_start, y_start, x_stop, y_stop, diag)\n",
    "\n",
    "    folder_str = 'Data/'\n",
    "    file_name = str(1)\n",
    "    name_svg = folder_str + file_name + \".svg\"\n",
    "    name_png = folder_str + file_name + \".png\"\n",
    "\n",
    "    new_file = open(name_svg, \"wt\")\n",
    "    new_file.write(string_svg)\n",
    "    new_file.close()\n",
    "\n",
    "    cairosvg.svg2png(url=name_svg, write_to=name_png)\n",
    "\n",
    "    stimulus_image = np.array(Image.open(name_png).convert('L'))\n",
    "\n",
    "\n",
    "    #get convolved responses\n",
    "\n",
    "#     cluster_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,]\n",
    "    dsi = np.zeros(13)\n",
    "    dsi_off = np.zeros(13)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "    for i, clust in enumerate(ipl_depth_order):\n",
    "        # get the necessaries\n",
    "        # spline = (NoiseSplineRF2 & key).fetch1('spl')\n",
    "        spline = average_filters[:,:,clust] #average_filters_mirror[:, :, clust] 4:24\n",
    "        edge_buffer = 15 #(ConvolutionKernelParams() & key).fetch1('edge_buffer')\n",
    "        # stimulus_image = (ConvolutionStimulus() & key).fetch1('stimulus_image')\n",
    "        stimulus_image_rev = np.flip(stimulus_image, axis=0)\n",
    "        kernel_type = 'left'#(ConvolutionKernelParams() & key).fetch1('kernel_type')\n",
    "        # space_per_pixelx, space_per_pixely = (StimulusParams.NoiseParams() & key).fetch1('box_dx_um', 'box_dy_um')\n",
    "\n",
    "        # set up and crop the kernel\n",
    "        kernel_length = stimulus_image.shape[1]  # needs to be the same length as the stimulus\n",
    "\n",
    "        # get the location of the RF center in the kernel\n",
    "        spline_temp = spline[edge_buffer:-edge_buffer, :]\n",
    "        index_max = np.unravel_index(np.argmax(np.abs(spline_temp), axis=None), spline_temp.shape)\n",
    "        peak_y = index_max[1]\n",
    "#         print(peak_y)\n",
    "        peak_y = 19\n",
    "\n",
    "        # crop the kernel depending on the type of experiment\n",
    "        if kernel_type == 'right':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer, np.int(peak_y - 1):np.int(peak_y + kernel_length - 1)]\n",
    "        elif kernel_type == 'left':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer, np.int(peak_y + 1 - kernel_length):np.int(peak_y + 1)]\n",
    "        elif kernel_type == 'full':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "                         0:kernel_length]  # should be full kernel, just adding space range just in case\n",
    "        elif kernel_type == 'centered':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "                         np.int(np.floor(peak_y - kernel_length / 2)):np.int(np.floor(peak_y + kernel_length / 2))]\n",
    "        else:\n",
    "            spline_cut = spline  # just to make the code happy, this should never be the option\n",
    "        # can code in other possibilities here...\n",
    "\n",
    "        # check whether cut spline is big enough\n",
    "        space_length = spline_cut.shape[1]\n",
    "        if kernel_length == space_length:\n",
    "            empty_flag = 0\n",
    "            # setup output variables\n",
    "            convolved_response = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "            convolved_response_rev = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "\n",
    "            # loop through x positions and convolve\n",
    "            for x_position in range(stimulus_image.shape[1]):\n",
    "                ker = np.flip(spline_cut[:, x_position])\n",
    "                img = stimulus_image[:, x_position]\n",
    "                img2 = stimulus_image_rev[:, x_position]\n",
    "                # and then convolve these two\n",
    "                convolved_response[:, x_position] = np.convolve(a=img, v=ker, mode='full')\n",
    "                convolved_response_rev[:, x_position] = np.convolve(a=img2, v=ker, mode='full')\n",
    "        else:\n",
    "            empty_flag = 1\n",
    "            convolved_response = []\n",
    "            convolved_response_rev = []\n",
    "\n",
    "        response = np.sum(convolved_response, axis=1)\n",
    "        response_rev = np.sum(convolved_response_rev, axis=1)\n",
    "        max_response = np.amax(response)\n",
    "        max_response_rev = np.amax(response_rev)\n",
    "        max_response_off = np.amin(response)\n",
    "        max_response_rev_off = np.amin(response_rev)\n",
    "        dsi[clust] = (max_response - max_response_rev) / (max_response + max_response_rev)\n",
    "#         print(max_response, max_response_rev)\n",
    "        dsi_off[clust] = (max_response_off - max_response_rev_off) / (max_response_off + max_response_rev_off)\n",
    "#         dsi_all = np.where(cluster_avg_depth<0.5, dsi, dsi_off)\n",
    "    #     print(dsi, dsi_off)\n",
    "    #     plt.figure()\n",
    "        ax[i].plot(response*polarity[clust], color='b')\n",
    "        ax[i].plot(response_rev*polarity[clust], color='k')\n",
    "        ax[i].axhline(color='k', linestyle='dotted')\n",
    "    #     ax[i].set_yticklabels([])\n",
    "    #     ax[i].set_xticklabels([])\n",
    "    #     ax[i].axis('off')\n",
    "\n",
    "    # Figpath = 'Fig_temp/'\n",
    "    # savename = Figpath+\"Convolved_responses.pdf\"\n",
    "    # plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "    return dsi, dsi_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run models\n",
    "velocities = [250, 350, 500, 750, 1000, 1250, 1500, 1750, 2000]\n",
    "\n",
    "dsi_all_all = np.zeros((len(velocities), 13))\n",
    "\n",
    "for i, velocity in enumerate(velocities):\n",
    "    dsi, dsi_off = convolved_dsis(velocity, convolution_params)\n",
    "    dsi_all_all[i, :] = np.where(cluster_avg_depth<0.5, dsi, dsi_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def convolved_dsis_centered(velocity, convolution_params):\n",
    "    # velocity tuning\n",
    "\n",
    "    # get the stimulus for convolution\n",
    "\n",
    "    convolution_params['stimulus_velocity'] = velocity # WARNING altering this to play around with velocity tuning\n",
    "    convolution_params['dendrite_length'] = 260\n",
    "    convolution_params['distance_covered'] = 260\n",
    "    if space_per_pixelx < space_per_pixely:  # check which is the shorter dimension of the stimulus\n",
    "        space_per_pixel = space_per_pixelx\n",
    "    else:\n",
    "        space_per_pixel = space_per_pixely\n",
    "\n",
    "    time, dend, diag, x_start, x_stop, y_start, y_stop = get_svg_parameters_on_screen(\n",
    "        convolution_params['dendrite_length'],\n",
    "        convolution_params['moving_bar_length'],\n",
    "        convolution_params['stimulus_velocity'],\n",
    "        convolution_params['starting_point'],\n",
    "        convolution_params['distance_covered'],\n",
    "        real_time_lag,\n",
    "        time_points,\n",
    "        space_per_pixel\n",
    "    )\n",
    "\n",
    "    string_svg = \"\"\"<svg width=\"{0}\" height=\"{1}\" viewBox=\"0 0 {2} {3}\">\n",
    "        <rect x=\"0\" y=\"0\" width=\"{4}\" height=\"{5}\" fill=\"black\" />\n",
    "          <line x1=\"{6}\" y1=\"{7}\" x2=\"{8}\" y2=\"{9}\"\n",
    "              stroke-width=\"{10}\" stroke=\"white\" stroke-linecap=\"square\"/>\n",
    "        </svg>\"\"\".format(dend, time, dend, time, dend, time, x_start, y_start, x_stop, y_stop, diag)\n",
    "\n",
    "    folder_str = 'Data/'\n",
    "    file_name = str(1)\n",
    "    name_svg = folder_str + file_name + \".svg\"\n",
    "    name_png = folder_str + file_name + \".png\"\n",
    "\n",
    "    new_file = open(name_svg, \"wt\")\n",
    "    new_file.write(string_svg)\n",
    "    new_file.close()\n",
    "\n",
    "    cairosvg.svg2png(url=name_svg, write_to=name_png)\n",
    "\n",
    "    stimulus_image = np.array(Image.open(name_png).convert('L'))\n",
    "\n",
    "\n",
    "    #get convolved responses\n",
    "\n",
    "#     cluster_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,]\n",
    "    dsi = np.zeros(13)\n",
    "    dsi_off = np.zeros(13)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "    for i, clust in enumerate(ipl_depth_order):\n",
    "        # get the necessaries\n",
    "        spline = average_filters[:,13:26,clust]\n",
    "        edge_buffer = 15 \n",
    "        stimulus_image_rev = np.flip(stimulus_image, axis=0)\n",
    "        kernel_type = 'full'#(ConvolutionKernelParams() & key).fetch1('kernel_type')\n",
    "\n",
    "        # set up and crop the kernel\n",
    "        kernel_length = stimulus_image.shape[1]  # needs to be the same length as the stimulus\n",
    "\n",
    "        # get the location of the RF center in the kernel\n",
    "        spline_temp = spline[edge_buffer:-edge_buffer, :]\n",
    "        index_max = np.unravel_index(np.argmax(np.abs(spline_temp), axis=None), spline_temp.shape)\n",
    "        peak_y = index_max[1]\n",
    "        peak_y = 19\n",
    "\n",
    "        # crop the kernel depending on the type of experiment\n",
    "        if kernel_type == 'right':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer, np.int(peak_y - 1):np.int(peak_y + kernel_length - 1)]\n",
    "        elif kernel_type == 'left':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer, np.int(peak_y + 1 - kernel_length):np.int(peak_y + 1)]\n",
    "        elif kernel_type == 'full':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "                         0:kernel_length]  # should be full kernel, just adding space range just in case\n",
    "        elif kernel_type == 'centered':\n",
    "#             spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "#                          4:11]\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "                         np.int(peak_y - (kernel_length-1) / 2):np.int(peak_y + (kernel_length-1) / 2 +1)]\n",
    "#             print(kernel_length)\n",
    "        else:\n",
    "            spline_cut = spline  # just to make the code happy, this should never be the option\n",
    "        # can code in other possibilities here...\n",
    "\n",
    "        # check whether cut spline is big enough\n",
    "        space_length = spline_cut.shape[1]\n",
    "        if kernel_length == space_length:\n",
    "            empty_flag = 0\n",
    "            # setup output variables\n",
    "            convolved_response = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "            convolved_response_rev = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "\n",
    "            # loop through x positions and convolve\n",
    "            for x_position in range(stimulus_image.shape[1]):\n",
    "                ker = np.flip(spline_cut[:, x_position])\n",
    "                img = stimulus_image[:, x_position]\n",
    "                img2 = stimulus_image_rev[:, x_position]\n",
    "                # and then convolve these two\n",
    "                convolved_response[:, x_position] = np.convolve(a=img, v=ker, mode='full')\n",
    "                convolved_response_rev[:, x_position] = np.convolve(a=img2, v=ker, mode='full')\n",
    "        else:\n",
    "            empty_flag = 1\n",
    "            convolved_response = []\n",
    "            convolved_response_rev = []\n",
    "\n",
    "        response = np.sum(convolved_response, axis=1)\n",
    "        response_rev = np.sum(convolved_response_rev, axis=1)\n",
    "        max_response = np.amax(response)\n",
    "        max_response_rev = np.amax(response_rev)\n",
    "        max_response_off = np.amin(response)\n",
    "        max_response_rev_off = np.amin(response_rev)\n",
    "        dsi[clust] = (max_response - max_response_rev) / (max_response + max_response_rev)\n",
    "#         print(max_response, max_response_rev)\n",
    "        dsi_off[clust] = (max_response_off - max_response_rev_off) / (max_response_off + max_response_rev_off)\n",
    "#         dsi_all = np.where(cluster_avg_depth<0.5, dsi, dsi_off)\n",
    "    #     print(dsi, dsi_off)\n",
    "    #     plt.figure()\n",
    "        ax[i].plot(response*polarity[clust], color='b')\n",
    "        ax[i].plot(response_rev*polarity[clust], color='k')\n",
    "        ax[i].axhline(color='k', linestyle='dotted')\n",
    "        ax[i].set_yticklabels([])\n",
    "        ax[i].set_xticklabels([])\n",
    "        ax[i].axis('off')\n",
    "#         if velocity == 1000:\n",
    "#             Figpath = 'Fig_temp/'\n",
    "#             savename = Figpath+\"Convolved_responses_1000.pdf\"\n",
    "#             plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "    return dsi, dsi_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run models\n",
    "velocities = [250, 350, 500, 750, 1000, 1250, 1500, 1750, 2000]\n",
    "\n",
    "dsi_all_all_centered = np.zeros((len(velocities), 13))\n",
    "\n",
    "for i, velocity in enumerate(velocities):\n",
    "    dsi, dsi_off = convolved_dsis_centered(velocity, convolution_params)\n",
    "    dsi_all_all_centered[i, :] = np.where(cluster_avg_depth<0.5, dsi, dsi_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#DSI as a function of velocity for each stimulus case\n",
    "fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "for i, clust in enumerate(ipl_depth_order):\n",
    "\n",
    "    ax[i].plot(velocities, dsi_all_all[:, clust]*-1, color='k')\n",
    "    ax[i].plot(velocities, dsi_all_all_centered[:, clust]*-1, color='grey')\n",
    "    ax[i].set_ylim([-.4, .72])\n",
    "    ax[i].axhline(color='k', linestyle='dotted')\n",
    "    ax[i].set_yticklabels([])\n",
    "    ax[i].set_xticklabels([])\n",
    "#     ax[i].axis('off')\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Velocity_tuning_clusters_both.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def convolved_dsis_drug(velocity, convolution_params):\n",
    "    # velocity tuning\n",
    "\n",
    "    # get the stimulus for convolution\n",
    "\n",
    "    convolution_params['stimulus_velocity'] = velocity # WARNING altering this to play around with velocity tuning\n",
    "    convolution_params['dendrite_length'] = 140\n",
    "    convolution_params['distance_covered'] = 140\n",
    "    if space_per_pixelx < space_per_pixely:  # check which is the shorter dimension of the stimulus\n",
    "        space_per_pixel = space_per_pixelx\n",
    "    else:\n",
    "        space_per_pixel = space_per_pixely\n",
    "\n",
    "    time, dend, diag, x_start, x_stop, y_start, y_stop = get_svg_parameters_on_screen(\n",
    "        convolution_params['dendrite_length'],\n",
    "        convolution_params['moving_bar_length'],\n",
    "        convolution_params['stimulus_velocity'],\n",
    "        convolution_params['starting_point'],\n",
    "        convolution_params['distance_covered'],\n",
    "        real_time_lag,\n",
    "        time_points,\n",
    "        space_per_pixel\n",
    "    )\n",
    "\n",
    "    string_svg = \"\"\"<svg width=\"{0}\" height=\"{1}\" viewBox=\"0 0 {2} {3}\">\n",
    "        <rect x=\"0\" y=\"0\" width=\"{4}\" height=\"{5}\" fill=\"black\" />\n",
    "          <line x1=\"{6}\" y1=\"{7}\" x2=\"{8}\" y2=\"{9}\"\n",
    "              stroke-width=\"{10}\" stroke=\"white\" stroke-linecap=\"square\"/>\n",
    "        </svg>\"\"\".format(dend, time, dend, time, dend, time, x_start, y_start, x_stop, y_stop, diag)\n",
    "\n",
    "    folder_str = 'Data/'\n",
    "    file_name = str(1)\n",
    "    name_svg = folder_str + file_name + \".svg\"\n",
    "    name_png = folder_str + file_name + \".png\"\n",
    "\n",
    "    new_file = open(name_svg, \"wt\")\n",
    "    new_file.write(string_svg)\n",
    "    new_file.close()\n",
    "\n",
    "    cairosvg.svg2png(url=name_svg, write_to=name_png)\n",
    "\n",
    "    stimulus_image = np.array(Image.open(name_png).convert('L'))\n",
    "\n",
    "\n",
    "    #get convolved responses\n",
    "\n",
    "#     cluster_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,]\n",
    "    dsi = np.zeros(13)\n",
    "    dsi_off = np.zeros(13)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "    for i, clust in enumerate(ipl_depth_order):\n",
    "        # get the necessaries\n",
    "        spline = average_filters_drug[:,:,clust] #average_filters_mirror[:, :, clust] 4:24\n",
    "        edge_buffer = 15 #\n",
    "        stimulus_image_rev = np.flip(stimulus_image, axis=0)\n",
    "        kernel_type = 'left'#\n",
    "\n",
    "        # set up and crop the kernel\n",
    "        kernel_length = stimulus_image.shape[1]  # needs to be the same length as the stimulus\n",
    "\n",
    "        # get the location of the RF center in the kernel\n",
    "        spline_temp = spline[edge_buffer:-edge_buffer, :]\n",
    "        index_max = np.unravel_index(np.argmax(np.abs(spline_temp), axis=None), spline_temp.shape)\n",
    "        peak_y = index_max[1]\n",
    "#         print(peak_y)\n",
    "        peak_y = 19\n",
    "\n",
    "        # crop the kernel depending on the type of experiment\n",
    "        if kernel_type == 'right':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer, np.int(peak_y - 1):np.int(peak_y + kernel_length - 1)]\n",
    "        elif kernel_type == 'left':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer, np.int(peak_y + 1 - kernel_length):np.int(peak_y + 1)]\n",
    "        elif kernel_type == 'full':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "                         0:kernel_length]  # should be full kernel, just adding space range just in case\n",
    "        elif kernel_type == 'centered':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "                         np.int(np.floor(peak_y - kernel_length / 2)):np.int(np.floor(peak_y + kernel_length / 2))]\n",
    "        else:\n",
    "            spline_cut = spline  # just to make the code happy, this should never be the option\n",
    "        # can code in other possibilities here...\n",
    "\n",
    "        # check whether cut spline is big enough\n",
    "        space_length = spline_cut.shape[1]\n",
    "        if kernel_length == space_length:\n",
    "            empty_flag = 0\n",
    "            # setup output variables\n",
    "            convolved_response = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "            convolved_response_rev = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "\n",
    "            # loop through x positions and convolve\n",
    "            for x_position in range(stimulus_image.shape[1]):\n",
    "                ker = np.flip(spline_cut[:, x_position])\n",
    "                img = stimulus_image[:, x_position]\n",
    "                img2 = stimulus_image_rev[:, x_position]\n",
    "                # and then convolve these two\n",
    "                convolved_response[:, x_position] = np.convolve(a=img, v=ker, mode='full')\n",
    "                convolved_response_rev[:, x_position] = np.convolve(a=img2, v=ker, mode='full')\n",
    "        else:\n",
    "            empty_flag = 1\n",
    "            convolved_response = []\n",
    "            convolved_response_rev = []\n",
    "\n",
    "        response = np.sum(convolved_response, axis=1)\n",
    "        response_rev = np.sum(convolved_response_rev, axis=1)\n",
    "        max_response = np.amax(response)\n",
    "        max_response_rev = np.amax(response_rev)\n",
    "        max_response_off = np.amin(response)\n",
    "        max_response_rev_off = np.amin(response_rev)\n",
    "        dsi[clust] = (max_response - max_response_rev) / (max_response + max_response_rev)\n",
    "#         print(max_response, max_response_rev)\n",
    "        dsi_off[clust] = (max_response_off - max_response_rev_off) / (max_response_off + max_response_rev_off)\n",
    "#         dsi_all = np.where(cluster_avg_depth<0.5, dsi, dsi_off)\n",
    "    #     print(dsi, dsi_off)\n",
    "    #     plt.figure()\n",
    "        ax[i].plot(response*polarity[clust], color='b')\n",
    "        ax[i].plot(response_rev*polarity[clust], color='k')\n",
    "        ax[i].axhline(color='k', linestyle='dotted')\n",
    "    #     ax[i].set_yticklabels([])\n",
    "    #     ax[i].set_xticklabels([])\n",
    "    #     ax[i].axis('off')\n",
    "\n",
    "    # Figpath = 'Fig_temp/'\n",
    "    # savename = Figpath+\"Convolved_responses.pdf\"\n",
    "    # plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "    return dsi, dsi_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run models\n",
    "velocities = [250, 350, 500, 750, 1000, 1250, 1500, 1750, 2000]\n",
    "\n",
    "dsi_all_all_drug = np.zeros((len(velocities), 13))\n",
    "\n",
    "for i, velocity in enumerate(velocities):\n",
    "    dsi, dsi_off = convolved_dsis_drug(velocity, convolution_params)\n",
    "    dsi_all_all_drug[i, :] = np.where(cluster_avg_depth<0.5, dsi, dsi_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def convolved_dsis_centered_drug(velocity, convolution_params):\n",
    "    # velocity tuning\n",
    "\n",
    "    # get the stimulus for convolution\n",
    "\n",
    "    convolution_params['stimulus_velocity'] = velocity # WARNING altering this to play around with velocity tuning\n",
    "    convolution_params['dendrite_length'] = 260\n",
    "    convolution_params['distance_covered'] = 260\n",
    "    if space_per_pixelx < space_per_pixely:  # check which is the shorter dimension of the stimulus\n",
    "        space_per_pixel = space_per_pixelx\n",
    "    else:\n",
    "        space_per_pixel = space_per_pixely\n",
    "\n",
    "    time, dend, diag, x_start, x_stop, y_start, y_stop = get_svg_parameters_on_screen(\n",
    "        convolution_params['dendrite_length'],\n",
    "        convolution_params['moving_bar_length'],\n",
    "        convolution_params['stimulus_velocity'],\n",
    "        convolution_params['starting_point'],\n",
    "        convolution_params['distance_covered'],\n",
    "        real_time_lag,\n",
    "        time_points,\n",
    "        space_per_pixel\n",
    "    )\n",
    "\n",
    "    string_svg = \"\"\"<svg width=\"{0}\" height=\"{1}\" viewBox=\"0 0 {2} {3}\">\n",
    "        <rect x=\"0\" y=\"0\" width=\"{4}\" height=\"{5}\" fill=\"black\" />\n",
    "          <line x1=\"{6}\" y1=\"{7}\" x2=\"{8}\" y2=\"{9}\"\n",
    "              stroke-width=\"{10}\" stroke=\"white\" stroke-linecap=\"square\"/>\n",
    "        </svg>\"\"\".format(dend, time, dend, time, dend, time, x_start, y_start, x_stop, y_stop, diag)\n",
    "\n",
    "    folder_str = 'Data/'\n",
    "    file_name = str(1)\n",
    "    name_svg = folder_str + file_name + \".svg\"\n",
    "    name_png = folder_str + file_name + \".png\"\n",
    "\n",
    "    new_file = open(name_svg, \"wt\")\n",
    "    new_file.write(string_svg)\n",
    "    new_file.close()\n",
    "\n",
    "    cairosvg.svg2png(url=name_svg, write_to=name_png)\n",
    "\n",
    "    stimulus_image = np.array(Image.open(name_png).convert('L'))\n",
    "\n",
    "\n",
    "    #get convolved responses\n",
    "\n",
    "#     cluster_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,]\n",
    "    dsi = np.zeros(13)\n",
    "    dsi_off = np.zeros(13)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "    for i, clust in enumerate(ipl_depth_order):\n",
    "        # get the necessaries\n",
    "\n",
    "        spline = average_filters_drug[:,13:26,clust]\n",
    "        edge_buffer = 15 \n",
    "\n",
    "        stimulus_image_rev = np.flip(stimulus_image, axis=0)\n",
    "        kernel_type = 'full'\n",
    "\n",
    "        # set up and crop the kernel\n",
    "        kernel_length = stimulus_image.shape[1]  # needs to be the same length as the stimulus\n",
    "\n",
    "        # get the location of the RF center in the kernel\n",
    "        spline_temp = spline[edge_buffer:-edge_buffer, :]\n",
    "        index_max = np.unravel_index(np.argmax(np.abs(spline_temp), axis=None), spline_temp.shape)\n",
    "        peak_y = index_max[1]\n",
    "        peak_y = 19\n",
    "\n",
    "        # crop the kernel depending on the type of experiment\n",
    "        if kernel_type == 'right':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer, np.int(peak_y - 1):np.int(peak_y + kernel_length - 1)]\n",
    "        elif kernel_type == 'left':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer, np.int(peak_y + 1 - kernel_length):np.int(peak_y + 1)]\n",
    "        elif kernel_type == 'full':\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "                         0:kernel_length]  # should be full kernel, just adding space range just in case\n",
    "        elif kernel_type == 'centered':\n",
    "#             spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "#                          4:11]\n",
    "            spline_cut = spline[edge_buffer:-edge_buffer,\n",
    "                         np.int(peak_y - (kernel_length-1) / 2):np.int(peak_y + (kernel_length-1) / 2 +1)]\n",
    "#             print(kernel_length)\n",
    "        else:\n",
    "            spline_cut = spline  # just to make the code happy, this should never be the option\n",
    "        # can code in other possibilities here...\n",
    "\n",
    "        # check whether cut spline is big enough\n",
    "        space_length = spline_cut.shape[1]\n",
    "        if kernel_length == space_length:\n",
    "            empty_flag = 0\n",
    "            # setup output variables\n",
    "            convolved_response = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "            convolved_response_rev = np.zeros((spline_cut.shape[0] + stimulus_image.shape[0] - 1, spline_cut.shape[1]))\n",
    "\n",
    "            # loop through x positions and convolve\n",
    "            for x_position in range(stimulus_image.shape[1]):\n",
    "                ker = np.flip(spline_cut[:, x_position])\n",
    "                img = stimulus_image[:, x_position]\n",
    "                img2 = stimulus_image_rev[:, x_position]\n",
    "                # and then convolve these two\n",
    "                convolved_response[:, x_position] = np.convolve(a=img, v=ker, mode='full')\n",
    "                convolved_response_rev[:, x_position] = np.convolve(a=img2, v=ker, mode='full')\n",
    "        else:\n",
    "            empty_flag = 1\n",
    "            convolved_response = []\n",
    "            convolved_response_rev = []\n",
    "\n",
    "        response = np.sum(convolved_response, axis=1)\n",
    "        response_rev = np.sum(convolved_response_rev, axis=1)\n",
    "        max_response = np.amax(response)\n",
    "        max_response_rev = np.amax(response_rev)\n",
    "        max_response_off = np.amin(response)\n",
    "        max_response_rev_off = np.amin(response_rev)\n",
    "        dsi[clust] = (max_response - max_response_rev) / (max_response + max_response_rev)\n",
    "#         print(max_response, max_response_rev)\n",
    "        dsi_off[clust] = (max_response_off - max_response_rev_off) / (max_response_off + max_response_rev_off)\n",
    "#         dsi_all = np.where(cluster_avg_depth<0.5, dsi, dsi_off)\n",
    "    #     print(dsi, dsi_off)\n",
    "    #     plt.figure()\n",
    "        ax[i].plot(response*polarity[clust], color='b')\n",
    "        ax[i].plot(response_rev*polarity[clust], color='k')\n",
    "        ax[i].axhline(color='k', linestyle='dotted')\n",
    "        ax[i].set_yticklabels([])\n",
    "        ax[i].set_xticklabels([])\n",
    "        ax[i].axis('off')\n",
    "#         if velocity == 1000:\n",
    "#             Figpath = 'Fig_temp/'\n",
    "#             savename = Figpath+\"Convolved_responses_1000.pdf\"\n",
    "#             plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "    return dsi, dsi_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run models\n",
    "velocities = [250, 350, 500, 750, 1000, 1250, 1500, 1750, 2000]\n",
    "\n",
    "dsi_all_all_centered_drug = np.zeros((len(velocities), 13))\n",
    "\n",
    "for i, velocity in enumerate(velocities):\n",
    "    dsi, dsi_off = convolved_dsis_centered_drug(velocity, convolution_params)\n",
    "    dsi_all_all_centered_drug[i, :] = np.where(cluster_avg_depth<0.5, dsi, dsi_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#DSI as a function of velocity for each stimulus case\n",
    "fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "for i, clust in enumerate(ipl_depth_order):\n",
    "\n",
    "    ax[i].plot(velocities, dsi_all_all_drug[:, clust]*-1, color='k')\n",
    "    ax[i].plot(velocities, dsi_all_all_centered_drug[:, clust]*-1, color='grey')\n",
    "    ax[i].set_ylim([-.4, .72])\n",
    "    ax[i].axhline(color='k', linestyle='dotted')\n",
    "    ax[i].set_yticklabels([])\n",
    "    ax[i].set_xticklabels([])\n",
    "#     ax[i].axis('off')\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Velocity_tuning_clusters_both.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#DSI as a function of velocity for each stimulus case\n",
    "fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "for i, clust in enumerate(ipl_depth_order):\n",
    "\n",
    "    ax[i].plot(velocities, dsi_all_all[:, clust]*-1, color='k')\n",
    "    ax[i].plot(velocities, dsi_all_all_drug[:, clust]*-1, color='r')\n",
    "    ax[i].set_ylim([-.4, .4])\n",
    "    ax[i].axhline(color='k', linestyle='dotted')\n",
    "    ax[i].set_yticklabels([])\n",
    "    ax[i].set_xticklabels([])\n",
    "#     ax[i].axis('off')\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Velocity_tuning_clusters_both.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#DSI as a function of velocity for each stimulus case\n",
    "fig, ax = plt.subplots(1, 13, figsize=(20, 1.6666))\n",
    "for i, clust in enumerate(ipl_depth_order):\n",
    "\n",
    "    ax[i].plot(velocities, dsi_all_all_centered[:, clust]*-1, color='k')\n",
    "    ax[i].plot(velocities, dsi_all_all_centered_drug[:, clust]*-1, color='r')\n",
    "    ax[i].set_ylim([-.25, .25])\n",
    "    ax[i].axhline(color='k', linestyle='dotted')\n",
    "    ax[i].set_yticklabels([])\n",
    "    ax[i].set_xticklabels([])\n",
    "#     ax[i].axis('off')\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Velocity_tuning_clusters_both.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion responses by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull these from hdf5\n",
    "dsi_results_44 = pd.read_hdf('Data/FigS08.hdf5', 'dsi_results_44')\n",
    "dsi_results_55 = pd.read_hdf('Data/FigS08.hdf5', 'dsi_results_55')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi_results_all = pd.concat([dsi_results_44, dsi_results_55])\n",
    "\n",
    "plt.hist(dsi_results_all['mu_std_max'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#remove non-responsive pixels, where the model failed to detect a large change in response\n",
    "threshold = 0.1\n",
    "dsi_results_all = dsi_results_all[dsi_results_all['mu_std_max']>threshold]\n",
    "dsi_results_all.pop('mu_std_max')\n",
    "\n",
    "std_100 = dsi_results_all['dsi_100'].std()\n",
    "std_150 = dsi_results_all['dsi_150'].std()\n",
    "std_200 = dsi_results_all['dsi_200'].std()\n",
    "std_300 = dsi_results_all['dsi_300'].std()\n",
    "std_100d = dsi_results_all['dsi_100d'].std()\n",
    "std_150d = dsi_results_all['dsi_150d'].std()\n",
    "std_200d = dsi_results_all['dsi_200d'].std()\n",
    "std_300d = dsi_results_all['dsi_300d'].std()\n",
    "\n",
    "mean_dsi = dsi_results_all['d_prime'].mean()\n",
    "dsi_results_all['dsi_diff'] = dsi_results_all['dsi_100d']*-1 - dsi_results_all['dsi_100']*-1\n",
    "\n",
    "std_dsi = np.array([std_100, std_150, std_200, std_300])\n",
    "std_dsi_d = np.array([std_100d, std_150d, std_200d, std_300d])\n",
    "\n",
    "std_top = mean_dsi + std_dsi\n",
    "std_bottom = mean_dsi - std_dsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dsi_results_all['cluster_center'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_boxplot = pd.melt(dsi_results_all[dsi_results_all['cluster_center'].isin(\n",
    "    [9,10])].reset_index(), id_vars=['index', 'cluster_assignment', 'field'],\n",
    "                      value_vars=['dsi_100', 'dsi_100d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_boxplot['value']=for_boxplot['value']*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_depth_order_on = [0,  3,  9,  7, 11,  2,  8]#,  5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10,5))\n",
    "sns.catplot(x=\"cluster_assignment\", y=\"value\", hue=\"variable\",order=ipl_depth_order_on, data=for_boxplot,\n",
    "                capsize=.05, palette=['k','red'],  markersize=2,linewidth=1,height=4, aspect=10/4,\n",
    "                kind=\"point\", ci=95,\n",
    "               \n",
    "               )\n",
    "\n",
    "axes = plt.gca()\n",
    "\n",
    "axes.axhline(linestyle='dotted', color='k')\n",
    "axes.set_ylim([-20,20])\n",
    "\n",
    "\n",
    "\n",
    "# Figpath = 'Fig_temp/'\n",
    "# savename = Figpath+\"Motion_responses_control_Strych_CI95.pdf\"\n",
    "# plt.savefig(savename, transparent=True, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stats: 2-way repeated measures Anova\n",
    "import pingouin as pg\n",
    "#need to remove the off clusters for testing\n",
    "for_stats = for_boxplot[for_boxplot['cluster_assignment'].isin(ipl_depth_order_on)]\n",
    "for_stats[\"id\"] = for_stats['index']+for_stats[\"field\"]*10000\n",
    "pg.rm_anova(dv='value',\n",
    "                  within=['cluster_assignment', 'variable'],\n",
    "                  subject='id', data=for_stats, correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pg.pairwise_ttests(dv='value', within='cluster_assignment', subject='id',\n",
    "                              between='variable', padjust='bonf', data=for_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
